#### 概述

通过 media 服务查看注册服务的过程

#### media 服务注册

[源码地址](http://aospxref.com/android-7.1.2_r39/xref/frameworks/av/media/mediaserver/main_mediaserver.cpp)

[参考文章地址](http://gityuan.com/2015/11/14/binder-add-service/)

```c++
int main(int argc __unused, char** argv)
{
    ...
    InitializeIcuOrDie();
    // 获得 ProcessState 实例对象
    sp<ProcessState> proc(ProcessState::self());
    // 获取BpServiceManager对象
    sp<IServiceManager> sm = defaultServiceManager();
    AudioFlinger::instantiate();
    // 注册多媒体服务
    MediaPlayerService::instantiate();
    ResourceManagerService::instantiate();
    CameraService::instantiate();
    AudioPolicyService::instantiate();
    SoundTriggerHwService::instantiate();
    RadioService::instantiate();
    registerExtensions();
    // 启动 Binder 线程池
    ProcessState::self()->startThreadPool();
    // 当前线程加入到线程池
    IPCThreadState::self()->joinThreadPool();
 }
```

##### mmap 方法参数

```c++
void* mmap(void* addr, size_t size, int prot, int flags, int fd, off_t offset) 

// 此处 mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0);
```

参数说明：

- addr: 代表映射到进程地址空间的起始地址，当值等于 0 则由内核选择合适地址，此处为 0；
- size: 代表需要映射的内存地址空间的大小，此处为1M-8K；
- prot: 代表内存映射区的读写等属性值，此处为PROT_READ(可读取);
- flags: 标志位，此处为MAP_PRIVATE(私有映射，多进程间不共享内容的改变)和 MAP_NORESERVE(不保留交换空间)
- fd: 代表 mmap 所关联的文件描述符，此处为 mDriverFD；
- offset：偏移量，此处为0。

####   注册多媒体服务过程

```c++
void MediaPlayerService::instantiate() {
    defaultServiceManager()->addService(String16("media.player"), new MediaPlayerService());
}
```

由 defaultServiceManager 返回的是BpServiceManager，同时会创建 ProcessState 对象和 BpBinder 对象。 故此处等价于调用 BpServiceManager->addService。其中 MediaPlayerService 位于 libmediaplayerservice 库.

##### BpServiceManager.addService 过程

```c++
defaultServiceManager()->addService(String16("media.player"), new MediaPlayerService());

virtual status_t addService(const String16& name, const sp<IBinder>& service, bool allowIsolated) {
    Parcel data, reply; // Parcel是数据通信包
    // 写入头信息"android.os.IServiceManager"
    data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());   
    data.writeString16(name);        // name为 "media.player"
    data.writeStrongBinder(service); // MediaPlayerService 对象
    data.writeInt32(allowIsolated ? 1 : 0); // allowIsolated= false
    // remote()指向的是BpBinder对象
    status_t err = remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply);
    return err == NO_ERROR ? reply.readExceptionCode() : err;
}
```

##### Parcel::writeStrongBinder 过程

```c++
data.writeStrongBinder(service); // MediaPlayerService 对象

status_t Parcel::writeStrongBinder(const sp<IBinder>& val)
{
    return flatten_binder(ProcessState::self(), val, this);
}

status_t flatten_binder(const sp<ProcessState>& /*proc*/, const sp<IBinder>& binder, Parcel* out)
{
    flat_binder_object obj;

    obj.flags = 0x7f | FLAT_BINDER_FLAG_ACCEPTS_FDS;
    if (binder != NULL) {
        IBinder *local = binder->localBinder(); // 本地Binder不为空
        if (!local) {
            BpBinder *proxy = binder->remoteBinder();
            const int32_t handle = proxy ? proxy->handle() : 0;
            obj.type = BINDER_TYPE_HANDLE;
            obj.binder = 0;
            obj.handle = handle;
            obj.cookie = 0;
        } else { // 进入该分支
            obj.type = BINDER_TYPE_BINDER;
            obj.binder = reinterpret_cast<uintptr_t>(local->getWeakRefs());
            obj.cookie = reinterpret_cast<uintptr_t>(local);
        }
    } else {
        ...
    }

    return finish_flatten_binder(binder, obj, out);
}

inline static status_t finish_flatten_binder(
    const sp<IBinder>& , const flat_binder_object& flat, Parcel* out)
{
    return out->writeObject(flat, false);
}
```

将 Binder 对象扁平化，转换成 flat_binder_object 对象。

- 对于 Binder 实体，则 cookie记录 Binder 实体的指针；
- 对于 Binder 代理，则用 handle 记录 Binder 代理的句柄；

##### BpBinder.transact 过程

writeStrongBinder 之后，接着执行 BPServiceManager.addService 方法中的

remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply)

其中 remote() 为 BpBinder(0)

```c++
status_t err = remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply);

status_t BpBinder::transact(
    uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags)
{
    if (mAlive) {
        // cod e= ADD_SERVICE_TRANSACTION
        status_t status = IPCThreadState::self()->transact(
            mHandle, code, data, reply, flags);
        if (status == DEAD_OBJECT) mAlive = 0;
        return status;
    }
    return DEAD_OBJECT;
}
```

Binder 代理类调用 transact() 方法，真正工作还是交给 IPCThreadState 来进行 transact 工作。先来 看看IPCThreadState::self 的过程。

##### IPCProcessState::self 过程

```c++
status_t status = IPCThreadState::self()->transact(mHandle, code, data, reply, flags);

IPCThreadState* IPCThreadState::self()
{
    if (gHaveTLS) {
restart:
        const pthread_key_t k = gTLS;
        IPCThreadState* st = (IPCThreadState*)pthread_getspecific(k);
        if (st) return st;
        return new IPCThreadState;  // 初始IPCThreadState 
    }

    if (gShutdown) return NULL;

    pthread_mutex_lock(&gTLSMutex);
    if (!gHaveTLS) { // 首次进入 gHaveTLS 为 false
        if (pthread_key_create(&gTLS, threadDestructor) != 0) { // 创建线程的TLS
            pthread_mutex_unlock(&gTLSMutex);
            return NULL;
        }
        gHaveTLS = true;
    }
    pthread_mutex_unlock(&gTLSMutex);
    goto restart;
}
```

TLS 是指 Thread local storage (线程本地储存空间)，每个线程都拥有自己的 TLS，并且是私有空间，线程之间不会共享。通过 pthread_getspecific/pthread_setspecific 函数可以获取/设置这些空间中的内容。从线程本地存储空间中获得保存在其中的 IPCThreadState对象。

##### IPCThreadState 初始化

```c++
IPCThreadState::IPCThreadState()
    : mProcess(ProcessState::self()),
      mMyThreadId(gettid()),
      mStrictModePolicy(0),
      mLastTransactionBinderFlags(0)
{
    pthread_setspecific(gTLS, this);
    clearCaller();
    mIn.setDataCapacity(256);
    mOut.setDataCapacity(256);
}
```

每个线程都有一个 IPCThreadState，每个 IPCThreadState 都有一个 mIn 和 mOut。并且成员变量 mProcess

保存 ProcessState 对象

##### IPCThreadState::transact 过程

```c++
status_t status = IPCThreadState::self()->transact(mHandle, code, data, reply, flags);

status_t IPCThreadState::transact(int32_t handle,
                                  uint32_t code, const Parcel& data,
                                  Parcel* reply, uint32_t flags)
{
    status_t err = data.errorCheck(); // 数据错误检查
    flags |= TF_ACCEPT_FDS;
    ....
    if (err == NO_ERROR) { 
        err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL);
    }
    ...

    if ((flags & TF_ONE_WAY) == 0) {
        if (reply) {
            //等待响应
            err = waitForResponse(reply);
        } else {
            Parcel fakeReply;
            err = waitForResponse(&fakeReply);
        }

    } else {
        //oneway，则不需要等待reply的场景
        err = waitForResponse(NULL, NULL);
    }
    return err;
}
```

##### writeTransactionData 过程

```c++
err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL);

status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t binderFlags,
    int32_t handle, uint32_t code, const Parcel& data, status_t* statusBuffer)
{
    binder_transaction_data tr;
    tr.target.ptr = 0;
    tr.target.handle = handle; // handle = 0
    tr.code = code;            // code = ADD_SERVICE_TRANSACTION
    tr.flags = binderFlags;    // binderFlags = 0
    tr.cookie = 0;
    tr.sender_pid = 0;
    tr.sender_euid = 0;

    // data 为记录 Media 服务信息的 Parcel 对象
    const status_t err = data.errorCheck();
    if (err == NO_ERROR) {
        tr.data_size = data.ipcDataSize();  // mDataSize
        tr.data.ptr.buffer = data.ipcData(); // mData
        tr.offsets_size = data.ipcObjectsCount()*sizeof(binder_size_t); // mObjectsSize
        tr.data.ptr.offsets = data.ipcObjects(); // mObjects
    } else if (statusBuffer) {
        ...
    } else {
        return (mLastError = err);
    }

    mOut.writeInt32(cmd);         // cmd = BC_TRANSACTION
    mOut.write(&tr, sizeof(tr));  // 写入 binder_transaction_data 数据
    return NO_ERROR;
}
```

其中 handle 的值用来标识目的端，注册服务过程的目的端为 service manager，此处 handle=0 所对应的是binder_context_mgr_node 对象，正是 service manager 所对应的 binder 实体对象。binder_transaction_data 结构体是binder 驱动通信的数据结构，该过程最终是把 Binder 请求码 BC_TRANSACTION 和 binder_transaction_data 结构体写入到`mOut`。

transact 过程，先写完 binder_transaction_data 数据，其中 Parcel data 的重要成员变量：

- mDataSize:保存 data_size，binder_transaction的数据大小；
- mData: 保存在 ptr.buffer, binder_transaction 的数据的起始地址；
- mObjectsSize:保存在ptr.offsets_size，记录着flat_binder_object结构体的个数；
- mObjects: 保存在offsets, 记录着flat_binder_object结构体在数据偏移量；

##### waitForResponse 过程

```c++
status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult)
{
    int32_t cmd;
    int32_t err;
    while (1) {
        if ((err=talkWithDriver()) < NO_ERROR) break;
        ...
        if (mIn.dataAvail() == 0) continue;

        cmd = mIn.readInt32();
        switch (cmd) {
            case BR_TRANSACTION_COMPLETE: ...
            case BR_DEAD_REPLY: ...
            case BR_FAILED_REPLY: ...
            case BR_ACQUIRE_RESULT: ...
            case BR_REPLY: ...
                goto finish;

            default:
                err = executeCommand(cmd); 
                if (err != NO_ERROR) goto finish;
                break;
        }
    }
    ...
    return err;
}
```

在 waitForResponse 过程, 首先执行 BR_TRANSACTION_COMPLETE；另外，目标进程收到事务后，处理BR_TRANSACTION 事务。 然后发送给当前进程，再执行BR_REPLY命令。

##### talkWithDriver 过程

```c++
status_t IPCThreadState::talkWithDriver(bool doReceive)
{
    ...
    binder_write_read bwr;
    const bool needRead = mIn.dataPosition() >= mIn.dataSize();
    const size_t outAvail = (!doReceive || needRead) ? mOut.dataSize() : 0;

    bwr.write_size = outAvail;
    bwr.write_buffer = (uintptr_t)mOut.data();

    if (doReceive && needRead) {
        // 接收数据缓冲区信息的填充。如果以后收到数据，就直接填在 mIn 中了。
        bwr.read_size = mIn.dataCapacity();
        bwr.read_buffer = (uintptr_t)mIn.data();
    } else {
        bwr.read_size = 0;
        bwr.read_buffer = 0;
    }
    // 当读缓冲和写缓冲都为空，则直接返回
    if ((bwr.write_size == 0) && (bwr.read_size == 0)) return NO_ERROR;

    bwr.write_consumed = 0;
    bwr.read_consumed = 0;
    status_t err;
    do {
        // 通过ioctl 不停的读写操作，跟 Binder Driver进行通信
        if (ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)
            err = NO_ERROR;
        ...
    } while (err == -EINTR); // 当被中断，则继续执行
    ...
    return err;
}
```

#### 总结

服务注册过程 (addService) 核心功能：在服务所在进程创建 binder_node，在 servicemanager 进程创建binder_ref。 其中 binder_ref 的 desc 再同一个进程内是唯一的：

- 每个进程 binder_proc 所记录的 binder_ref 的 handle 值是从1开始递增的；
- 所有进程 binder_proc 所记录的 handle=0 的 binder_ref 都指向service manager；
- 同一个服务的 binder_node 在不同进程的 binder_ref 的 handle 值可以不同；

<img src="/Users/xinwa/Library/Application Support/typora-user-images/image-20220218014245291.png" alt="image-20220218014245291" style="zoom:67%;" />

过程分析：

1. MediaPlayerService进程调用 ioctl() 向Binder驱动发送IPC数据，该过程可以理解成一个事务 binder_transaction

   (记为T1)，执行当前操作的线程binder_thread(记为thread1)，则T1->from_parent=NULL，

   T1->from = thread1，thread1->transaction_stack=T1。其中IPC数据内容包含：

   - Binder协议为BC_TRANSACTION；
   - Handle等于0；
   - RPC代码为ADD_SERVICE；
   - RPC数据为”media.player”。

2. Binder驱动收到该Binder请求，生成`BR_TRANSACTION`命令，选择目标处理该请求的线程，即ServiceManager的binder线程(记为`thread2`)，则 T1->to_parent = NULL，T1->to_thread = `thread2`。并将整个binder_transaction数据(记为`T2`)插入到目标线程的todo队列；

3. Service Manager的线程`thread2`收到`T2`后，调用服务注册函数将服务”media.player”注册到服务目录中。当服务注册完成后，生成IPC应答数据(`BC_REPLY`)，T2->form_parent = T1，T2->from = thread2, thread2->transaction_stack = T2。

4. Binder驱动收到该Binder应答请求，生成`BR_REPLY`命令，T2->to_parent = T1，T2->to_thread = thread1, thread1->transaction_stack = T2。 在MediaPlayerService收到该命令后，知道服务注册完成便可以正常使用。

<img src="/Users/xinwa/Library/Application Support/typora-user-images/image-20220218014714594.png" alt="image-20220218014714594" style="zoom:80%;" />

